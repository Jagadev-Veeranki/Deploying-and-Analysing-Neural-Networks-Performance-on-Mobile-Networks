{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQxpiMd0TaY8",
        "outputId": "2c1224de-e5c1-40a7-c0ea-b2b64e75d86e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-model-optimization\n",
            "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/242.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/242.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (0.1.8)\n",
            "Requirement already satisfied: numpy~=1.23 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.25.2)\n",
            "Requirement already satisfied: six~=1.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.16.0)\n",
            "Installing collected packages: tensorflow-model-optimization\n",
            "Successfully installed tensorflow-model-optimization-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-model-optimization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "class DefaultBNQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
        "    def get_weights_and_quantizers(self, layer):\n",
        "        return []\n",
        "\n",
        "    def get_activations_and_quantizers(self, layer):\n",
        "        return []\n",
        "\n",
        "    def set_quantize_weights(self, layer, quantize_weights):\n",
        "        pass\n",
        "\n",
        "    def set_quantize_activations(self, layer, quantize_activations):\n",
        "        pass\n",
        "\n",
        "    def get_output_quantizers(self, layer):\n",
        "        return []\n",
        "\n",
        "    def get_config(self):\n",
        "        return {}\n",
        "\n",
        "# Load and prepare data\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.astype('float32') / 255.0\n",
        "test_images = test_images.astype('float32') / 255.0\n",
        "\n",
        "with tf.keras.utils.custom_object_scope({'DefaultBNQuantizeConfig': DefaultBNQuantizeConfig}):\n",
        "    # Model architecture with quantization-aware training\n",
        "    input_layer = keras.layers.Input(shape=(28, 28))\n",
        "    x = keras.layers.Flatten()(input_layer)\n",
        "    x = keras.layers.Dense(1024, activation='relu')(x)\n",
        "    x = tfmot.quantization.keras.quantize_annotate_layer(\n",
        "        keras.layers.BatchNormalization(), DefaultBNQuantizeConfig())(x)\n",
        "    x = keras.layers.Dropout(0.5)(x)\n",
        "    x = keras.layers.Dense(1024, activation='relu')(x)\n",
        "    x = tfmot.quantization.keras.quantize_annotate_layer(\n",
        "        keras.layers.BatchNormalization(), DefaultBNQuantizeConfig())(x)\n",
        "    x = keras.layers.Dropout(0.5)(x)\n",
        "    output_layer = keras.layers.Dense(10, activation='softmax')(x)\n",
        "    model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    # Apply quantization to the model\n",
        "    quantize_model = tfmot.quantization.keras.quantize_apply(model)\n",
        "\n",
        "# Compile and train\n",
        "quantize_model.compile(optimizer='adam',\n",
        "                       loss='sparse_categorical_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "quantize_model.fit(train_images, train_labels, epochs=10, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "quantize_model.evaluate(test_images, test_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvpThhbvUBy1",
        "outputId": "b3f29613-5401-4f0e-f141-0f8d44611421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "Epoch 1/10\n",
            "1688/1688 [==============================] - 77s 34ms/step - loss: 0.3684 - accuracy: 0.8939 - val_loss: 0.1046 - val_accuracy: 0.9702\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 58s 34ms/step - loss: 0.2118 - accuracy: 0.9364 - val_loss: 0.1080 - val_accuracy: 0.9723\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 57s 34ms/step - loss: 0.1840 - accuracy: 0.9444 - val_loss: 0.0934 - val_accuracy: 0.9727\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 57s 34ms/step - loss: 0.1606 - accuracy: 0.9516 - val_loss: 0.0756 - val_accuracy: 0.9800\n",
            "Epoch 5/10\n",
            "1688/1688 [==============================] - 57s 34ms/step - loss: 0.1495 - accuracy: 0.9543 - val_loss: 0.0770 - val_accuracy: 0.9788\n",
            "Epoch 6/10\n",
            "1688/1688 [==============================] - 56s 33ms/step - loss: 0.1378 - accuracy: 0.9577 - val_loss: 0.0711 - val_accuracy: 0.9832\n",
            "Epoch 7/10\n",
            "1688/1688 [==============================] - 58s 34ms/step - loss: 0.1244 - accuracy: 0.9623 - val_loss: 0.0679 - val_accuracy: 0.9815\n",
            "Epoch 8/10\n",
            "1688/1688 [==============================] - 56s 33ms/step - loss: 0.1145 - accuracy: 0.9646 - val_loss: 0.0737 - val_accuracy: 0.9797\n",
            "Epoch 9/10\n",
            "1688/1688 [==============================] - 57s 34ms/step - loss: 0.1096 - accuracy: 0.9665 - val_loss: 0.0609 - val_accuracy: 0.9843\n",
            "Epoch 10/10\n",
            "1688/1688 [==============================] - 57s 34ms/step - loss: 0.1009 - accuracy: 0.9689 - val_loss: 0.0655 - val_accuracy: 0.9818\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0610 - accuracy: 0.9820\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.061024781316518784, 0.9819999933242798]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vQc3GD1asDML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "# Load MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "train_images = train_images.astype('float32') / 255.0\n",
        "test_images = test_images.astype('float32') / 255.0\n",
        "\n",
        "# Prepare labels for precision and recall calculations\n",
        "test_labels_cat = keras.utils.to_categorical(test_labels)\n",
        "\n",
        "# Model creation\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(300, activation='relu'),\n",
        "    keras.layers.Dense(100, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Define model pruning parameters\n",
        "pruning_params = {\n",
        "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.10,\n",
        "                                                             final_sparsity=0.80,\n",
        "                                                             begin_step=2000,\n",
        "                                                             end_step=6000)\n",
        "}\n",
        "\n",
        "# Apply pruning to the whole model\n",
        "pruned_model = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
        "\n",
        "# Compile the pruned model\n",
        "pruned_model.compile(optimizer='adam',\n",
        "                     loss='sparse_categorical_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "\n",
        "# Callback for pruning\n",
        "callbacks = [\n",
        "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "    tfmot.sparsity.keras.PruningSummaries(log_dir='/tmp/logs')\n",
        "]\n",
        "\n",
        "# Train the pruned model\n",
        "pruned_model.fit(train_images, train_labels,\n",
        "                 epochs=10,\n",
        "                 validation_data=(test_images, test_labels),\n",
        "                 callbacks=callbacks)\n",
        "\n",
        "# Evaluate the pruned model\n",
        "result = pruned_model.evaluate(test_images, test_labels, verbose=0)\n",
        "predicted_labels = pruned_model.predict(test_images)\n",
        "predicted_labels = predicted_labels.argmax(axis=1)\n",
        "\n",
        "# Calculate precision and recall\n",
        "precision = precision_score(test_labels, predicted_labels, average='macro')\n",
        "recall = recall_score(test_labels, predicted_labels, average='macro')\n",
        "\n",
        "print(f\"Precision: {precision}, Recall: {recall}\")\n",
        "\n",
        "# Apply quantization\n",
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "quantized_model = quantize_model(pruned_model)\n",
        "\n",
        "# Compile the quantized model\n",
        "quantized_model.compile(optimizer='adam',\n",
        "                        loss='sparse_categorical_crossentropy',\n",
        "                        metrics=['accuracy'])\n",
        "\n",
        "# Evaluate the quantized model\n",
        "quantized_model.evaluate(test_images, test_labels)\n",
        "\n",
        "# Convert to TensorFlow Lite model with Huffman Coding\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(quantized_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "# Save the model\n",
        "with open('compressed_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_quant_model)\n",
        "\n",
        "print(\"Model has been compressed and is ready for deployment.\")\n"
      ],
      "metadata": {
        "id": "bmQauTpcsDkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update"
      ],
      "metadata": {
        "id": "jm_4OUh7ymF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "# Load MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "train_images = train_images.astype('float32') / 255.0\n",
        "test_images = test_images.astype('float32') / 255.0\n",
        "\n",
        "# Model creation\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(300, activation='relu'),\n",
        "    keras.layers.Dense(100, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Define model pruning parameters\n",
        "pruning_params = {\n",
        "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.10,\n",
        "                                                             final_sparsity=0.80,\n",
        "                                                             begin_step=2000,\n",
        "                                                             end_step=6000)\n",
        "}\n",
        "\n",
        "# Apply pruning to the whole model\n",
        "pruned_model = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
        "\n",
        "# Train the pruned model\n",
        "pruned_model.compile(optimizer='adam',\n",
        "                     loss='sparse_categorical_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "\n",
        "callbacks = [\n",
        "    tfmot.sparsity.keras.UpdatePruningStep()\n",
        "]\n",
        "\n",
        "pruned_model.fit(train_images, train_labels,\n",
        "                 epochs=10,\n",
        "                 validation_data=(test_images, test_labels),\n",
        "                 callbacks=callbacks)\n",
        "\n",
        "# Strip the pruning wrappers for quantization compatibility\n",
        "model_for_quantization = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
        "\n",
        "# Quantize the model\n",
        "with tfmot.quantization.keras.quantize_scope():\n",
        "    # Apply quantization to the whole model\n",
        "    quantized_model = tfmot.quantization.keras.quantize_model(model_for_quantization)\n",
        "\n",
        "quantized_model.compile(optimizer='adam',\n",
        "                        loss='sparse_categorical_crossentropy',\n",
        "                        metrics=['accuracy'])\n",
        "\n",
        "# Train and evaluate the quantized model\n",
        "quantized_model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))\n",
        "result = quantized_model.evaluate(test_images, test_labels)\n",
        "\n",
        "# Output precision and recall\n",
        "predicted_labels = quantized_model.predict(test_images)\n",
        "predicted_labels = predicted_labels.argmax(axis=1)\n",
        "precision = precision_score(test_labels, predicted_labels, average='macro')\n",
        "recall = recall_score(test_labels, predicted_labels, average='macro')\n",
        "\n",
        "print(f\"Precision: {precision}, Recall: {recall}\")\n",
        "\n",
        "# Convert to TensorFlow Lite model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(quantized_model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model\n",
        "with open('compressed_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"Model has been compressed and is ready for deployment.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlhJEwjjySqp",
        "outputId": "ea146ce4-829a-47af-b8d7-4466bfb6d3a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 46s 8ms/step - loss: 0.2048 - accuracy: 0.9394 - val_loss: 0.1000 - val_accuracy: 0.9691\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 16s 8ms/step - loss: 0.0791 - accuracy: 0.9755 - val_loss: 0.0741 - val_accuracy: 0.9769\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0523 - accuracy: 0.9850 - val_loss: 0.0561 - val_accuracy: 0.9827\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0309 - accuracy: 0.9912 - val_loss: 0.0548 - val_accuracy: 0.9833\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0213 - accuracy: 0.9937 - val_loss: 0.0590 - val_accuracy: 0.9828\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0156 - accuracy: 0.9957 - val_loss: 0.0563 - val_accuracy: 0.9835\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0117 - accuracy: 0.9971 - val_loss: 0.0580 - val_accuracy: 0.9839\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0088 - accuracy: 0.9977 - val_loss: 0.0702 - val_accuracy: 0.9822\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0066 - accuracy: 0.9983 - val_loss: 0.0731 - val_accuracy: 0.9823\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0056 - accuracy: 0.9985 - val_loss: 0.0741 - val_accuracy: 0.9841\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 18s 8ms/step - loss: 0.0430 - accuracy: 0.9859 - val_loss: 0.0934 - val_accuracy: 0.9762\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0316 - accuracy: 0.9901 - val_loss: 0.0875 - val_accuracy: 0.9777\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0273 - accuracy: 0.9911 - val_loss: 0.0857 - val_accuracy: 0.9782\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 16s 8ms/step - loss: 0.0234 - accuracy: 0.9925 - val_loss: 0.0792 - val_accuracy: 0.9813\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0225 - accuracy: 0.9924 - val_loss: 0.1037 - val_accuracy: 0.9789\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 16s 8ms/step - loss: 0.0181 - accuracy: 0.9936 - val_loss: 0.1029 - val_accuracy: 0.9766\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0166 - accuracy: 0.9948 - val_loss: 0.1061 - val_accuracy: 0.9794\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0187 - accuracy: 0.9942 - val_loss: 0.1009 - val_accuracy: 0.9788\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0151 - accuracy: 0.9948 - val_loss: 0.0872 - val_accuracy: 0.9824\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0130 - accuracy: 0.9958 - val_loss: 0.1137 - val_accuracy: 0.9800\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1137 - accuracy: 0.9800\n",
            "313/313 [==============================] - 1s 3ms/step\n",
            "Precision: 0.9801092304971245, Recall: 0.9797923441097387\n",
            "Model has been compressed and is ready for deployment.\n"
          ]
        }
      ]
    }
  ]
}